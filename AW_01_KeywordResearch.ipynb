{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AW-01-KeywordResearch.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMFVmWBUuLSvzDOPtJfBzs3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanignaciorey/Autocontent_SEO_with_AUTOML/blob/main/AW_01_KeywordResearch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5RRjevRSKuR"
      },
      "source": [
        " keywords... piense en 5 a 6 cifras sin perder la calidad\n",
        "\n",
        "\n",
        " Tenemos que buscar la mayor cantidad de fuentes... sin perder el objetivo\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hWq_b-BgDuu"
      },
      "source": [
        "### 1. Requeriment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bK1SGGngBfm"
      },
      "source": [
        "### Requerimientos\n",
        "!pip install pandas\n",
        "!pip install splinter\n",
        "!conda install splinter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxrPMTyGi7bQ"
      },
      "source": [
        "#@title Instalacion de paquetes\n",
        "!pip install selenium\n",
        "!apt-get update # to update ubuntu to correctly run apt install\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_3qzxcwgyxz",
        "outputId": "57bb4ead-3638-482c-dce8-a5f3fed199ee"
      },
      "source": [
        "#@title Configuracion de Selenium Browser\n",
        "import sys\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "from selenium import webdriver\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "browser = webdriver.Chrome('chromedriver',chrome_options=chrome_options)\n",
        "browser.get(\"https://www.webite-url.com\")\n",
        "## https://stackoverflow.com/questions/56829470/selenium-google-colab-error-chromedriver-executable-needs-to-be-in-path"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: use options instead of chrome_options\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sc5iVWHkEHg"
      },
      "source": [
        "width =   400 #@param {type:\"integer\"}\n",
        "height =   768 #@param {type:\"integer\"}\n",
        "# Rezising\n",
        "browser.set_window_size(width, height)\n",
        "browser.maximize_window()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luYQLIRJmZKA"
      },
      "source": [
        "#@title Instalacion de librerias\n",
        "import os\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "import time\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAloZvtHmevN"
      },
      "source": [
        "#@title Drive Mount\n",
        "# conecta tu tiempo de ejecución de Google Colab a tu Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDBTfbIsoOn-"
      },
      "source": [
        "### 2. Pilar Keywords\n",
        "\n",
        "Separa las palabras clave con ','"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R84LQt_BoTmX"
      },
      "source": [
        "kwywords = 'com' #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAENpXH3oBaP"
      },
      "source": [
        "### 3. Serp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2_zF2Ue2McC"
      },
      "source": [
        "try:\n",
        "    from googlesearch import search\n",
        "except ImportError: \n",
        "    print(\"No module named 'google' found\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "q89VmKptn_GG"
      },
      "source": [
        "# ref: https://www.geeksforgeeks.org/performing-google-search-using-python-code/\n",
        "# search(query, tld='com', lang='en', num=10, start=0, stop=None, pause=2.0)\n",
        "\n",
        "# tld stands for top level domain which means we want to search our result on google.com or google.in or some other domain.\n",
        "tld = 'com' #@param {type:\"string\"}\n",
        "# lang : lang stands for language. \n",
        "# https://www.labnol.org/code/19899-google-translate-languages\n",
        "lang = 'es' #@param ['es', 'en']\n",
        "# Number of results we want.\n",
        "num = 10 #@param {type:\"integer\"}\n",
        "# start : First result to retrieve.\n",
        "start = 0 \n",
        "# stop : Last result to retrieve. Use None to keep searching forever.\n",
        "stop = None\n",
        "# pause : Lapse to wait between HTTP requests. Lapse too short may cause Google to block your IP. Keeping significant lapse will make your program slow but its safe and better option.\n",
        "pause = 2.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGL8ub553Un8"
      },
      "source": [
        "# query : query string that we want to search for.\n",
        "query = \"download book * filetype:epub\"\n",
        "# Return : Generator (iterator) that yields found URLs. If the stop parameter is None the iterator will loop forever.\n",
        "for j in search(query, tld, lang, num, stop, pause):\n",
        "    print(j)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "cellView": "form",
        "id": "yf3X3sQkXuKQ",
        "outputId": "e779736f-710c-4f3f-ebdf-735a469fc8b5"
      },
      "source": [
        "#@title\n",
        "import os\n",
        "import re\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import datetime\n",
        "import platform\n",
        "from docopt import docopt\n",
        "from tqdm import tqdm \n",
        "from time import sleep\n",
        "import pandas as pd\n",
        "from pandas.io.json import json_normalize\n",
        "import logging\n",
        "from jinja2 import Environment, FileSystemLoader\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.common.action_chains import ActionChains\n",
        "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException\n",
        "\n",
        "''' \n",
        "Visualizza una barra di caricamento per mostrare l'attesa\n",
        "'''\n",
        "def sleepBar(seconds):\n",
        "    for i in tqdm(range(seconds)):\n",
        "        sleep(1)\n",
        "\n",
        "def prettyOutputName(filetype='html'):\n",
        "    _query = re.sub('\\s|\\\"|\\/|\\:|\\.','_', query.rstrip())\n",
        "    prettyname = _query\n",
        "    ts = time.time()\n",
        "    st = datetime.datetime.fromtimestamp(ts).strftime('%d-%m-%Y_%H-%M-%S-%f')\n",
        "    if filetype != 'html':\n",
        "        prettyname += \"_\" + st + \".\" + filetype\n",
        "    else:\n",
        "        prettyname += \"_\" + st + \".\" + filetype\n",
        "    return prettyname\n",
        "\n",
        "\n",
        "def initBrowser(headless=False):\n",
        "    if \"Windows\" in platform.system():\n",
        "        chrome_path = \"driver/chromedriver.exe\"\n",
        "    else:\n",
        "        chrome_path = \"driver/chromedriver\"\n",
        "    chrome_options = Options()\n",
        "    chrome_options.add_argument(\"--disable-features=NetworkService\")\n",
        "    if headless:\n",
        "        chrome_options.add_argument('headless')\n",
        "    return webdriver.Chrome(options=chrome_options,executable_path=chrome_path)\n",
        "\"\"\"\n",
        "Search on Google and returns the list of PAA questions in SERP.\n",
        "\"\"\"\n",
        "def newSearch(browser,query):\n",
        "    if lang== \"en\":\n",
        "        browser.get(\"https://www.google.com?hl=en\")\n",
        "        searchbox = browser.find_element_by_xpath(\"//input[@aria-label='Search']\")\n",
        "    else:\n",
        "        browser.get(\"https://www.google.com?hl=es\")\n",
        "        searchbox = browser.find_element_by_xpath(\"//input[@aria-label='Buscar']\")\n",
        "    \n",
        "    searchbox.send_keys(query)\n",
        "    sleepBar(2)\n",
        "    tabNTimes()\n",
        "    if lang== \"en\":\n",
        "        searchbtn = browser.find_elements_by_xpath(\"//input[@aria-label='Google Search']\")\n",
        "    else:\n",
        "    \tsearchbtn = browser.find_elements_by_xpath(\"//input[@aria-label='Buscar con Google']\")\n",
        "    try:\n",
        "        searchbtn[-1].click()\n",
        "    except:\n",
        "        searchbtn[0].click()\n",
        "    sleepBar(2)\n",
        "    paa = browser.find_elements_by_xpath(\"//span/following-sibling::div[contains(@class,'match-mod-horizontal-padding')]\")\n",
        "    hideGBar()\n",
        "    return paa\n",
        "\"\"\"\n",
        "Helper function that scroll into view the PAA questions element.\n",
        "\"\"\"\n",
        "def scrollToFeedback():\n",
        "    if lang == \"en\":\n",
        "        el = browser.find_element_by_xpath(\"//div[@class='kno-ftr']//div/following-sibling::a[text()='Feedback']\")\n",
        "    else:\n",
        "    \tel = browser.find_element_by_xpath(\"//div[@class='kno-ftr']//div/following-sibling::a[text()='Enviar comentarios']\")\n",
        "\n",
        "    actions = ActionChains(browser)\n",
        "    actions.move_to_element(el).perform()\n",
        "    browser.execute_script(\"arguments[0].scrollIntoView();\", el)\n",
        "    actions.send_keys(Keys.PAGE_UP).perform()\n",
        "    sleepBar(1)\n",
        "\"\"\"\n",
        "Accessibility helper: press TAB N times (default 2)\n",
        "\"\"\"\n",
        "def tabNTimes(N=2):\n",
        "    actions = ActionChains(browser) \n",
        "    for _ in range(N):\n",
        "        actions = actions.send_keys(Keys.TAB)\n",
        "    actions.perform()\n",
        "\n",
        "\"\"\"\n",
        "Click on questions N times\n",
        "\"\"\"\n",
        "def clickNTimes(el, n=1):\n",
        "    for i in range(n):\n",
        "        el.click()\n",
        "        logging.info(f\"clicking on ... {el.text}\")\n",
        "        sleepBar(1)\n",
        "        scrollToFeedback()\n",
        "        try:\n",
        "            el.find_element_by_xpath(\"//*[@aria-expanded='true']\").click()\n",
        "        except:\n",
        "            pass\n",
        "        sleepBar(1)\n",
        "\n",
        "\"\"\"\n",
        "Hide Google Bar to prevent ClickInterceptionError\n",
        "\"\"\"\n",
        "def hideGBar():\n",
        "\ttry:\n",
        "\t\tbrowser.execute_script('document.getElementById(\"searchform\").style.display = \"none\";')\n",
        "\texcept:\n",
        "\t\tpass\n",
        "\n",
        "\"\"\"\n",
        "Where the magic happens\n",
        "\"\"\"\n",
        "def crawlQuestions(start_paa, paa_list, initialSet, depth=0):\n",
        "    _tmp = createNode(paa_lst=paa_list, name=query, children=True)\n",
        "    \n",
        "    outer_cnt = 0\n",
        "    for q in start_paa:\n",
        "        scrollToFeedback()\n",
        "        if \"Dictionary\" in q.text:\n",
        "            continue\n",
        "        test = createNode(paa_lst=paa_list, n=0,\n",
        "                        name=q.text,\n",
        "                        parent=paa_list[0][\"name\"],\n",
        "                        children=True)\n",
        "        \n",
        "        clickNTimes(q)\n",
        "        new_q = showNewQuestions(initialSet)\n",
        "        for l, value in new_q.items():\n",
        "            sleepBar(1)\n",
        "            logging.info(f\"{l}, {value.text}\")\n",
        "            test1 = createNode(paa_lst=test[0][\"children\"][outer_cnt][\"children\"], \n",
        "                                name=value.text,\n",
        "                                parent=test[0][\"children\"][outer_cnt][\"name\"],\n",
        "                                children=True)\n",
        "            \n",
        "        initialSet = getCurrentSERP()\n",
        "        logging.info(f\"Current count: {outer_cnt}\")\n",
        "        outer_cnt += 1\n",
        "        if depth==1:\n",
        "            for i in range(depth):\n",
        "                currentQuestions = []\n",
        "                for i in initialSet.values():\n",
        "                    currentQuestions.append(i.text)\n",
        "                for i in paa_list[0][\"children\"]:\n",
        "                    for j in i[\"children\"]:\n",
        "                        parent = j['name']\n",
        "                        logging.info(parent)\n",
        "                        _tmpset = set()\n",
        "                        if parent in currentQuestions:\n",
        "                            try:\n",
        "                                if \"'\" in parent:\n",
        "                                    xpath_compiler = '//div[text()=\"' + parent + '\"]'\n",
        "                                else: \n",
        "                                    xpath_compiler= \"//div[text()='\" + parent + \"']\"\n",
        "                                question= browser.find_element_by_xpath(xpath_compiler)\n",
        "                            except NoSuchElementException:\n",
        "                                continue\n",
        "                            scrollToFeedback()\n",
        "                            sleepBar(1)\n",
        "                            clickNTimes(question)\n",
        "                            new_q = showNewQuestions(initialSet)\n",
        "                            for l, value in new_q.items():\n",
        "                                if value.text == parent:\n",
        "                                    continue\n",
        "                                j['children'].append({\"name\": value.text,\"parent\": parent})\n",
        "                                \n",
        "                            initialSet = getCurrentSERP()\n",
        "\n",
        "\"\"\"\n",
        "Get the current Result Page.\n",
        "Returns: \n",
        "    A list with newest questions.\n",
        "\"\"\"\n",
        "def getCurrentSERP():\n",
        "    _tmpset = {}\n",
        "    new_paa = browser.find_elements_by_xpath(\"//span/following-sibling::div[contains(@class,'match-mod-horizontal-padding')]\")\n",
        "    cnt= 0\n",
        "    for q in new_paa:\n",
        "        _tmpset.update({cnt:q})\n",
        "        cnt +=1\n",
        "    newInitialSet = _tmpset\n",
        "    return newInitialSet\n",
        "\n",
        "\"\"\"\n",
        "Shows new questions.\n",
        "Args:\n",
        "    intialSet (dict): The initial set in the PAA box.\n",
        "Returns:\n",
        "    list of questions with first 3-4 questions deleted (initalSet).\n",
        "\"\"\"\n",
        "def showNewQuestions(initialSet):\n",
        "    tmp = getCurrentSERP()\n",
        "    deletelist = [k for k, v in initialSet.items() if k in tmp and tmp[k] == v]\n",
        "    _tst = dict.copy(tmp)\n",
        "    for i,value in tmp.items():\n",
        "        if i in deletelist:\n",
        "            _tst.pop(i)\n",
        "    return _tst\n",
        "\n",
        "\"\"\"\n",
        "Create a new node in the list.\n",
        "Args:\n",
        "    paa_list_elements: list of web elements\n",
        "    n: index of 'children' list on a main node\n",
        "    name: node nome\n",
        "    parent: Indicates if the node has a parent. Default to null only for first level.\n",
        "    chilren: Indicates if the node has a children list. default false\n",
        "Returns:\n",
        "    list of questions with the current new node\n",
        "\"\"\"\n",
        "def createNode( n=-1, parent='null', children=False, name='',*, paa_lst):\n",
        "    logging.info(paa_lst)\n",
        "    if children:\n",
        "        _d = {\n",
        "        \"name\": name,\n",
        "        \"parent\": parent,\n",
        "        \"children\": [] \n",
        "        }\n",
        "    else:\n",
        "        _d = {\n",
        "        \"name\": name,\n",
        "        \"parent\": parent\n",
        "        }\n",
        "    if n!=-1:\n",
        "        logging.info(paa_lst[n][\"children\"])\n",
        "        paa_lst[n][\"children\"].append(_d)\n",
        "    else:\n",
        "        paa_lst.append(_d)\n",
        "    \n",
        "\n",
        "    return paa_lst\n",
        "\n",
        "\"\"\"\n",
        "This func takes in input JSON data and returns csv file.\n",
        "\"\"\"\n",
        "def flatten_csv(data,depth,prettyname):\n",
        "    try:\n",
        "        if depth == 0:\n",
        "            _ = json_normalize(data[0][\"children\"], 'children', ['name', 'parent',['children',]], record_prefix='inner.')\n",
        "            _.drop(columns=['children','inner.children','inner.parent'], inplace=True)\n",
        "            columnTitle = ['parent','name','inner.name']\n",
        "            _ = _.reindex(columns=columnTitle)\n",
        "            _.to_csv(prettyname,sep=\";\",encoding='utf-8')\n",
        "        elif depth == 1:\n",
        "            df = json_normalize(data[0][\"children\"], meta=['name','children','parent'], record_path=\"children\", record_prefix='inner.')\n",
        "            frames = [ json_normalize(i) for i in df['inner.children'] ]\n",
        "            result = pd.concat(frames)\n",
        "            result.rename(columns={\"name\": \"inner.inner.name\", \"parent\": \"inner.name\"}, inplace=True)\n",
        "            merge = pd.merge(df, result, how='left', on=\"inner.name\")\n",
        "            merge.drop(columns=['name'], inplace=True)\n",
        "            columnTitle = ['parent','inner.parent','inner.name','inner.inner.name']\n",
        "            merge = merge.reindex(columns=columnTitle)\n",
        "            merge = merge.drop_duplicates(subset='inner.inner.name', keep='first')\n",
        "            merge.to_csv(prettyname,sep=';',encoding='utf-8')\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"{e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logging.basicConfig(level=logging.INFO)\n",
        "    args = docopt(usage)\n",
        "    print(args)\n",
        "    MAX_DEPTH = 1\n",
        "\n",
        "    if args['<depth>']:\n",
        "        depth = int(args['<depth>'])\n",
        "        if depth > MAX_DEPTH:\n",
        "            sys.exit(\"depth not allowed\")\n",
        "    else:\n",
        "        depth = 0\n",
        "\n",
        "    if args['en']:\n",
        "        lang = \"en\"\n",
        "    elif args['es']:\n",
        "        lang = \"es\"\n",
        "        \n",
        "\n",
        "\n",
        "    if args['<keyword>']:\n",
        "        if args['--headless']:\n",
        "            browser = initBrowser(True)\n",
        "        else:\n",
        "            browser = initBrowser()\n",
        "        query = args['<keyword>']\n",
        "        start_paa = newSearch(browser,query)\n",
        "\n",
        "        initialSet = {}\n",
        "        cnt= 0\n",
        "        for q in start_paa:\n",
        "            initialSet.update({cnt:q})\n",
        "            cnt +=1\n",
        "\n",
        "        paa_list = []\n",
        "\n",
        "        crawlQuestions(start_paa, paa_list, initialSet,depth)\n",
        "        treeData = 'var treeData = ' + json.dumps(paa_list) + ';'\n",
        "        \n",
        "        if paa_list[0]['children']:\n",
        "            root = os.path.dirname(os.path.abspath(__file__))\n",
        "            templates_dir = os.path.join(root, 'templates')\n",
        "            env = Environment( loader = FileSystemLoader(templates_dir) )\n",
        "            template = env.get_template('index.html')\n",
        "            filename = os.path.join(root, 'html', prettyOutputName())\n",
        "            with open(filename, 'w') as fh:\n",
        "                fh.write(template.render(\n",
        "                    treeData = treeData,\n",
        "                ))\n",
        "\n",
        "    if args['--csv']:\n",
        "        if paa_list[0]['children']:\n",
        "            _path = 'csv/'+prettyOutputName('csv')\n",
        "            flatten_csv(paa_list, depth, _path)\n",
        "\n",
        "    browser.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-5b959f107891>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasicConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINFO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocopt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0mMAX_DEPTH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'usage' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9rrL2opUh_2"
      },
      "source": [
        "# python gquestions.py query <keyword> (en|es) [depth <depth>] [--csv] [--headless]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "axcpECGMUwvQ",
        "outputId": "de03c62e-8c51-4349-a4a4-dbe3aa1b57d1"
      },
      "source": [
        "form = Slider()\n",
        "form._repr_html_()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n    <input type=\"range\" id=\"in140498676663184\"\\n     min=\"0\" max=\"100\" step=\"1\"\\n     value=\"0\" \\n     oninput=\"out140498676663184.value=in140498676663184.value\"\\n     onchange=\"\\n       google.colab.kernel.invokeFunction(\\n         \\'set_var\\', [140498676663184, this.valueAsNumber],\\n         {})\\n     \"\\n    >\\n    <output id=\"out140498676663184\">0</output>\\n    '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqRtCuHlFmpZ"
      },
      "source": [
        "### 4. Trends"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ii8CjgUFveM"
      },
      "source": [
        "from pytrends.request import TrendReq\n",
        "import pandas as pd\n",
        "import time\n",
        "startTime = time.time()\n",
        "pytrend = TrendReq(hl='en-GB', tz=360)\n",
        "\n",
        "dataset = []\n",
        "keywords = query_list\n",
        "\n",
        "for x in range(0,len(df2)):\n",
        "     keywords = [df2[x]]\n",
        "     pytrend.build_payload(\n",
        "     kw_list=keywords,\n",
        "     cat=0,\n",
        "     timeframe='2020-04-01 2020-05-01',\n",
        "     geo='GB')\n",
        "     data = pytrend.interest_over_time()\n",
        "     if not data.empty:\n",
        "          data = data.drop(labels=['isPartial'],axis='columns')\n",
        "          dataset.append(data)\n",
        "\n",
        "result = pd.concat(dataset, axis=1)\n",
        "result.to_csv('search_trends.csv')\n",
        "\n",
        "executionTime = (time.time() - startTime)\n",
        "print('Execution time in sec.: ' + str(executionTime))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LH5v-Pwh4sL7"
      },
      "source": [
        "####  Ebooks\n",
        "\n",
        "##### Dorks\n",
        "name_of_ebook filetype:pdf\n",
        "name_of_ebook filetype:epub\n",
        "name_of_ebook filetype:mobi\n",
        "name_of_ebook filetype:txt\n",
        "intitle:index.of (epub)\n",
        "intitle:\"calibre library\" inurl:browse\n",
        "\n",
        "##### Webs\n",
        "https://www.overdrive.com/\n",
        "http://libgen.rs/\n",
        "https://www.gutenberg.org/\n",
        "https://centslessbooks.com/\n",
        "\n",
        "https://bookboon.com/es\n",
        "http://free-ebooks.net/\n",
        "https://freecomputerbooks.com/\n",
        "https://manybooks.net/\n",
        "\n",
        "##### Dispositivos\n",
        "Amazon kindle\n",
        "Audible\n",
        "Google Play books\n",
        "Kobo books"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upCCeMyj6RYT"
      },
      "source": [
        "nombres_posibles = []\n",
        "filetypes = ['pdf','epub','mobi','txt']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvHdxiIfoAXQ"
      },
      "source": [
        "### 2. Medium Posts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBLR9I4UlzRe"
      },
      "source": [
        "keyword = \"data%20science\"\n",
        "url = \"https://medium.com/search?q=\"+keyword\n",
        "print(url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_zKCf9xmztO"
      },
      "source": [
        "wd.get(url)\n",
        "\n",
        "ScrollNumber = 50\n",
        "for i in range(1,ScrollNumber):\n",
        "    wd.execute_script(\"window.scrollTo(1,50000)\")\n",
        "    time.sleep(5)\n",
        "\n",
        "file = open('DS.html', 'w')\n",
        "file.write(wd.page_source)\n",
        "file.close()\n",
        "\n",
        "wd.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxMm0WFDm4kY"
      },
      "source": [
        "import urllib3\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "http=urllib3.PoolManager()\n",
        "\n",
        "def scrape():\n",
        "    data = open('DS.html','r')\n",
        "    soup = BeautifulSoup(data, 'html.parser')\n",
        "    for links in soup.find_all('div', {'class': 'postArticle-readMore'}):\n",
        "        link = links.find('a').get('href')\n",
        "        Blog_post(link)\n",
        "\n",
        "def Blog_post(link):\n",
        "    try:\n",
        "        print(link)\n",
        "        blogData = http.request('GET', link)\n",
        "        soup = BeautifulSoup(blogData.data, 'html.parser')\n",
        "        article = ''\n",
        "        tags = []\n",
        "        heading = soup.find('h1').text\n",
        "        for para in soup.find_all('p'):\n",
        "            p = para.text\n",
        "        p = p.strip('/u')\n",
        "        article = article + ' ' + p\n",
        "        for mtags in soup.find_all('a', {'class ': 'link u - baseColor — link'}):\n",
        "            tags.append(mtags.text)\n",
        "            # CreateDataFrame(list())\n",
        "            someList = [heading, article, tuple(tags)]\n",
        "            # print(someList[0])\n",
        "            CreateDataFrame(someList)\n",
        "    except:\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ufaPbo9gaYb"
      },
      "source": [
        "search_bar.fill(\"serpstat.com\")\n",
        "# Now let's set up code to click the search button!\n",
        "search_button_xpath = '//*[@id=\"tsf\"]/div[2]/div[3]/center/input[1]'\n",
        "search_button = browser.find_by_xpath(search_button_xpath)[0]\n",
        "search_button.click()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORBAPU-eSMfR"
      },
      "source": [
        "### Google Also Ask\n",
        "\n",
        "### Google Sugest\n",
        "\n",
        "### Google Pin badage\n",
        "#### Analizar las primeras \n",
        "#### Analizar bbusquedas relacionadas desde acá\n",
        "##### https://www.google.com/search?q=descargar+gta+vice+city+espa%C3%B1ol&rlz=1C1CHBF_esAR932AR932&sxsrf=ALeKk01KrKANaiNvjzZaJH25ZiXnRYqEcA:1616707508424&source=lnms&tbm=isch&sa=X&ved=2ahUKEwjj1vrlsMzvAhXuIbkGHRV8A9wQ_AUoAnoECAEQBA&biw=1920&bih=937\n",
        "\n",
        "### Adwords\n",
        "\n",
        "### Translate - Para hacer canonical content\n",
        "\n",
        "### Keywords de top(competencia) del tema\n",
        "\n",
        "### "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxUg5ZsZKhTC",
        "outputId": "7ed074c4-d139-4300-fa65-b77f70b191de"
      },
      "source": [
        "def is_valid(mediafire_url):\n",
        "    \"\"\"\n",
        "    Check is the link to mediafire is no broken\n",
        "    Example of a broken link: 'https://www.mediafire.com/file/null/oretresf-01.mp4/file'\n",
        "    \"\"\"\n",
        "    valid = True\n",
        "    link_parts = mediafire_url.split(\"/\")\n",
        "    if (link_parts[4] == \"null\"):\n",
        "        valid = False\n",
        "\n",
        "    return valid\n",
        "\n",
        "is_valid = is_valid(\"url\")\n",
        "is_valid"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f3zLkqdQEVo"
      },
      "source": [
        "Deberiamos encontrar keywords desde distintas fuentes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUcsh5k3p6XO"
      },
      "source": [
        "!pip install tensorflow-gpu==1.13.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2B2zbAYp8mm"
      },
      "source": [
        "!pip install ludwig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzhYBMmtqAb4",
        "outputId": "78f8ce20-3944-4989-be13-632dcbeebc1e"
      },
      "source": [
        "!gsutil cp gs://dataset-uploader/bbc/bbc-text.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CommandException: Wrong number of arguments for \"cp\" command.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VGxTIhMqDKl"
      },
      "source": [
        "input_features:\n",
        "        name: text\n",
        "        type: text\n",
        "        level: word\n",
        "        encoder: parallel_cnn\n",
        "\n",
        "output_features:\n",
        "        name: category\n",
        "        type: category"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5963IDdri1c"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4myrpu-ZqhZg"
      },
      "source": [
        "!ludwig experiment \\\n",
        "  --data_csv bbc-text.csv\\\n",
        "  --model_definition_file model_definition.yaml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GVnbeBpqp5m"
      },
      "source": [
        "#https://github.com/uber/ludwig/issues/267#issuecomment-497304317\n",
        "from ludwig import visualize\n",
        "visualize.learning_curves(['/content/results/experiment_run_0/training_statistics.json'],None)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}